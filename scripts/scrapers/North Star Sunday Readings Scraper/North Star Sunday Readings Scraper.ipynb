{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb21883-de1f-4739-9e05-41c0da9cde0d",
   "metadata": {},
   "source": [
    "# North Star Sunday Readings Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c5214-8c54-4321-a1e7-158a301ef50a",
   "metadata": {},
   "source": [
    "This is a script to scrape, process and export the Sunday Readings in Chamorro from the North Star website, which is a publication of the diocese of Chalan Kanoa. The text will be 1) exported to an HTML file to be converted to other reading-friendly formats for easier access for learners; 2) split into sentences; and 3) split into words. This will make the text accessible for future analysis, research, corpus development, lexicon expansion and language learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505efae1-b2cd-4892-9a39-78f4e52817ec",
   "metadata": {},
   "source": [
    "**Name:** Schyuler Lujan <br>\n",
    "**Date Started:** 02-May-2025 <br>\n",
    "**Date Completed** 12-May-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4293fa6f-d6f7-42a1-be75-5d6c17c224dc",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "1adeea11-4683-44f7-98e8-124c6027c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for web scraping\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import libraries for exporting data\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Import libraries for tokenization and text cleanup\n",
    "from nltk import tokenize\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e225e8-2e81-4ed7-92c1-ba6595e79b2c",
   "metadata": {},
   "source": [
    "## Get URLs for the Navigation Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32600fd-fbbb-483a-9fd3-17a5278c0706",
   "metadata": {},
   "source": [
    "In this section, we will navigate to the webpage on the North Star website that allows us to navigate to each individual Sunday Reading and use `BeautifulSoup` to get all of the URLs for the navigation pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a39fa52-8fdd-4fe8-bf41-ed202218b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the URL for the main webpage for the Sunday Readings on the North Star website\n",
    "initial_url = 'https://northstar.website/category/sunday-readings-in-chamorro/'\n",
    "\n",
    "# Get the last page of the navigation pages by looking at the webpage\n",
    "last_page = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "5f8fb9b1-f2ef-49e6-87c3-7c8e1e7ad30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_navigation_page_links(url, last):\n",
    "    \"\"\"\n",
    "    Generate and return a list of URLs for navigating through all pages containing Sunday Readings in Chamorro.\n",
    "\n",
    "    This function constructs a list of URLs, starting with a given URL and appending additional page links\n",
    "    based on the provided total number of pages. The URLs in the list point to the pages containing links \n",
    "    to individual Sunday Readings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The URL of the first page from which navigation begins.\n",
    "    last : int\n",
    "        The total number of pages containing the Sunday Readings. The function will generate URLs for all pages \n",
    "        starting from the first up to the specified page number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    navigation_links: list of str\n",
    "        A list of URLs, each pointing to a page containing links to the individual Sunday Readings.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function assumes the navigation follows a consistent URL pattern, with pages numbered sequentially (e.g., `page/1/`, `page/2/`, etc.).\n",
    "    - The first URL is included in the list, followed by all subsequent pages up to the last specified page.\n",
    "    \"\"\"\n",
    "    # Initialize list to store navigation links\n",
    "    navigation_links = [url]\n",
    "\n",
    "    # Create the other navigation urls\n",
    "    for i in range(2,last+1):\n",
    "        next_url = url + f\"page/{i}/\"\n",
    "        navigation_links.append(next_url)\n",
    "\n",
    "    return navigation_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c6f6548-e9c1-473e-a6c9-1a442d0ab71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the other navigation links\n",
    "navigation_urls = get_navigation_page_links(initial_url, last_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c2dbf1-f434-45d2-ba7b-4e4ae9daac9f",
   "metadata": {},
   "source": [
    "## Get URLs for Sunday Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81539c4d-59bd-40c8-bc48-5085edd2446f",
   "metadata": {},
   "source": [
    "In this section, we will go to each navigation page for the Sunday Readings and use `BeautifulSoup` get the URL for each individual Sunday Reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "f31ae1b4-0b70-4dd6-8117-ea171c16fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sunday_readings_links(navigation_pages):\n",
    "    \"\"\"\n",
    "    Scrape and return a list of URLs to individual Sunday Readings in Chamorro.\n",
    "\n",
    "    This function takes a list of navigation page URLs, scrapes each page for links to individual Sunday Readings,\n",
    "    and returns a list of those URLs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    navigation_pages : list of str\n",
    "        A list of URLs pointing to pages containing links to individual Sunday Readings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reading_urls: list of str\n",
    "        A list of URLs, each pointing to an individual Sunday Reading in Chamorro.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function uses `requests` and `BeautifulSoup` to fetch and parse the HTML content of each page.\n",
    "    - Each navigation page is expected to contain links to individual readings within `<h2>` elements with the class `title`.\n",
    "    - A delay of 1 second is added between requests to avoid overloading the server.\n",
    "    - Errors encountered during HTTP requests or HTML parsing are caught and logged, but they do not interrupt the process.\n",
    "    \"\"\"\n",
    "    # Initialize list for storing Sunday Readings links\n",
    "    readings_urls = []\n",
    "\n",
    "    # Set headers to avoid 406 error\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/114.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
    "    }\n",
    "\n",
    "    # Iterate through the navigation urls and use BeautifulSoup to get the urls for each Sunday reading\n",
    "    for page in navigation_pages:\n",
    "        try:\n",
    "            response = requests.get(page, headers=headers, timeout=10)\n",
    "            response.raise_for_status() # Raise error for bad responses\n",
    "            response.encoding = response.apparent_encoding\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Find the links to each individual Sunday reading\n",
    "            links = soup.find_all(\"h2\", class_=\"title\")\n",
    "\n",
    "            # Extract the individual links\n",
    "            for link in links:\n",
    "                url = link.find(\"a\")\n",
    "                readings_urls.append(url[\"href\"])\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed for {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {url}: {e}\")\n",
    "\n",
    "        # Pause between pages\n",
    "        time.sleep(1)\n",
    "\n",
    "    return readings_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b36a4ed-ef56-47c1-b785-acf55e5c086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get links for each Sunday Reading\n",
    "readings_urls = get_sunday_readings_links(navigation_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "097031c1-c05a-4531-a490-34da694e8472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n"
     ]
    }
   ],
   "source": [
    "# View url count\n",
    "print(len(readings_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46492388-286a-486c-bd87-9bd501eea689",
   "metadata": {},
   "source": [
    "### Export Links List to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d493af5a-c443-443b-929e-d4af6e38ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the Sunday Readings urls to a CSV for future reference and use\n",
    "with open('urls_sunday_readings.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for url in readings_urls:\n",
    "        writer.writerow([url])  # wrap each string in a list so it’s one column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e2a77-88d9-4dfa-90b4-5ad4f23217f1",
   "metadata": {},
   "source": [
    "## Get Sunday Readings Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc0207-6adc-4ee8-a2e0-d46040023f11",
   "metadata": {},
   "source": [
    "In this section, we will use `BeautifulSoup` to scrape the reading metadata and text for each Sunday reading and store it in a Python dictionary. After examining the structure of each Sunday Reading page, these are the elements we will extract data from:\n",
    "\n",
    "* **Title:** `<h1 class=\"entry-title\">V DAMENGGU GI UTDINÅRIU NA TIEMPU</h1>` <br>\n",
    "* **Date:** `<span class=\"posted-on\">Posted on <a href=\"https://northstar.website/v-damenggu-gi-utdinariu-na-tiempu-8/\" rel=\"bookmark\"><span class=\"entry-date published\" datetime=\"2025-02-08T17:54:48+10:00\">February 8, 2025</span>` <br>\n",
    "* **Author:** `<span class=\"author vcard\"><a class=\"url fn n\" href=\"https://northstar.website/author/rita-guerrero/\">Rita Guerrero</a></span>` <br>\n",
    "* **Text Content:** The text content of the readings is found `<div class=\"page-content\">` And all the text is contained in `<p>` paragraph tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "2a39b4e5-9cc8-4d71-9a9f-073aea76ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(content_urls):\n",
    "    \"\"\"\n",
    "    Scrape and return a dictionary of text content and metadata for each Sunday Reading in Chamorro.\n",
    "\n",
    "    This function visits each URL in the input list, parses the HTML to extract the article's title, author,\n",
    "    date, and main text content, and stores the results in a structured dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content_urls : list of str\n",
    "        A list of URLs pointing to individual articles or readings to be scraped.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sunday_readings: dict\n",
    "        A dictionary where each key is a string in the format \"article_N\" (e.g., \"article_1\"),\n",
    "        and each value is another dictionary with the following keys:\n",
    "\n",
    "            - 'title' : str\n",
    "                The title of the article.\n",
    "            - 'author' : str\n",
    "                The author's name.\n",
    "            - 'date' : str\n",
    "                The publication date.\n",
    "            - 'url' : str\n",
    "                The URL of the article.\n",
    "            - 'text' : str\n",
    "                The main body text of the article, with paragraphs separated by double newlines.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function uses `requests` and `BeautifulSoup` to fetch and parse HTML content.\n",
    "    - If any metadata field is missing, \"N/A\" is used as a placeholder.\n",
    "    - A delay of 1 second is introduced between requests to be respectful to the server.\n",
    "    - Errors during fetching or parsing are caught and printed, but do not stop execution.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize dictionary for storing content\n",
    "    sunday_readings = {}\n",
    "\n",
    "    # Initialize counter for article naming convention\n",
    "    counter = 0\n",
    "\n",
    "    # Set headers to avoid 406 error\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/114.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
    "    }\n",
    "\n",
    "    # Iterate through urls for the Sunday readings, parse the HTML, get the metadata and text\n",
    "    for url in content_urls:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status() # Raise error for bad responses\n",
    "            response.encoding = response.apparent_encoding\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Get elements\n",
    "            title = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            date = soup.find('span', class_='entry-date published')\n",
    "            author = soup.find(class_=\"url fn n\")\n",
    "            text_div = soup.find(\"div\", class_=\"page-content\")\n",
    "\n",
    "            # If the tite, date and author elements are found, extract the text and convert to strings, otherwise return N/A\n",
    "            title = title.get_text(strip=True) if title else \"N/A\"\n",
    "            date = date.get_text() if date else \"N/A\"\n",
    "            author = author.get_text() if author else \"N/A\"  \n",
    "\n",
    "            # Get the text content\n",
    "            if text_div:\n",
    "                paragraphs = text_div.find_all(\"p\")\n",
    "                if paragraphs:\n",
    "                    content = \"\\n\\n\".join(p.get_text() for p in paragraphs)\n",
    "                else:\n",
    "                    content = text_div.get_text() if text_div else \"N/A\"\n",
    "\n",
    "            # Add reading metadata and text content to sunday_readings\n",
    "            counter += 1\n",
    "            sunday_readings[f\"article_{counter}\"] = {\n",
    "                \"title\":title, \n",
    "                \"author\":author, \n",
    "                \"date\":date, \n",
    "                \"url\":str(url), \n",
    "                \"text\":content\n",
    "            }\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed for {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {url}: {e}\")\n",
    "\n",
    "        # Pause between urls\n",
    "        time.sleep(1)\n",
    "            \n",
    "    return sunday_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc742949-e64e-40ac-a2a2-132c018848c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for https://northstar.website/damenggun-pinadesi-damenggun-ramus-i-likao-patma-5/: 404 Client Error: Not Found for url: https://northstar.website/damenggun-pinadesi-damenggun-ramus-i-likao-patma-5/\n"
     ]
    }
   ],
   "source": [
    "# Get Sunday Readings content\n",
    "contents = get_content(readings_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613fd16-7a3d-4c2b-8dbf-720c1f75e262",
   "metadata": {},
   "source": [
    "### Export Data to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7810e2d-d444-4b1e-8730-9e3ee3bbc0ef",
   "metadata": {},
   "source": [
    "The data will be exported as a JSON file for future analysis and data projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "279a4842-f81f-4210-b7d1-742fe4a39404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export contents to JSON file for future analysis and research\n",
    "with open('sunday_readings_in_chamorro.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(contents, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0bf85b-9111-4540-9954-cf35e4785c2e",
   "metadata": {},
   "source": [
    "## Format and Export Readings to HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ab49b-8091-45c0-ae7c-51a8ff5df11e",
   "metadata": {},
   "source": [
    "In this section, we will format the scraped content into an HTML format and export it to an HTML file, which can then be converted to other reading formats (i.e.: .PDF, .EPUB, .DOCX) to make reading and studying this content more accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dbda4ad-2e89-43ec-8ada-ec45aa9a46c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_content_as_html(text_content, work_title=\"Sunday Readings in Chamorro\"):\n",
    "    \"\"\"\n",
    "    Convert a dictionary of article content into a single HTML-formatted string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        text_content: dict\n",
    "             A dictionary where each key is an article ID and each value is another dictionary\n",
    "             containing 'title', 'author', 'date', 'url', and 'text' fields representing the article.\n",
    "        work_title: str, optional\n",
    "            The title to use for the HTML document's <title> tag (default is \"Sunday Readings in Chamorro\")\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        combined_html_content: str\n",
    "            A string containing all article content wrapped in HTML tags, with each article\n",
    "            formatted as a section including a heading, metadata, and paragraphs for the text body.\n",
    "    \"\"\"\n",
    "    # Initialize the HTML structure for formatting\n",
    "    combined_html_content = f\"\"\"\n",
    "    <html>\n",
    "    <head><meta charset = \"UTF-8\"><title>{work_title}</title></head>\n",
    "    <body>\n",
    "    \"\"\"\n",
    "    # Get the articles and append metadata and text contents to HTML structure\n",
    "    for article in text_content:\n",
    "        # Get the article metadata and text contents\n",
    "        title = text_content[article]['title']\n",
    "        author = text_content[article]['author']\n",
    "        date = text_content[article]['date']\n",
    "        url = text_content[article]['url']\n",
    "        text = text_content[article]['text']\n",
    "        \n",
    "        # Convert text with \\n\\n into HTML <p> paragraphs\n",
    "        paragraphs = ''.join(f\"<p>{para.strip()}</p>\\n\" for para in text.split('\\n\\n') if para.strip())\n",
    "        \n",
    "        # Append the content to the HTML structure\n",
    "        combined_html_content += f\"\"\"\n",
    "        <section>\n",
    "        <h1>{title}</h1>\n",
    "        <p><strong>Date:</strong> {date}<p>\n",
    "        <p><strong>Author:</strong> {author}<p>\n",
    "        <p><strong>URL:</strong> {url}<p>\n",
    "        {paragraphs}\n",
    "        </section>\n",
    "        <hr>\n",
    "        \"\"\"\n",
    "    # Close the HTML structure\n",
    "    combined_html_content += f\"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return combined_html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9eadaa2-8af0-488d-b48e-e549e00f5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HTML structure\n",
    "html_structure = format_content_as_html(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61966d30-e6ee-4b40-8a33-1b7cf92e8320",
   "metadata": {},
   "source": [
    "### Export Text to HTML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "489755e5-b2ca-4165-bb90-1b440c6f83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to HTML file\n",
    "with open(\"sunday_readings_in_chamorro.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fe75a-bcf1-40db-bc34-f8b9d879537c",
   "metadata": {},
   "source": [
    "# Get Sentences for Corpus Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c732b6-113b-445f-a0ab-9657afdb139d",
   "metadata": {},
   "source": [
    "In this section we will go through the text content of the Sunday Readings, split the content into individual sentences and export to a .csv file for the development of a Chamorro corpus.\n",
    "\n",
    "In the .csv export file, each sentence will also include its associated metadata (title, source name, date, url)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "c3b19072-1ae1-4ca5-aa0f-91d1045d95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(data, source_name=\"North Star Sunday Readings\"):\n",
    "    \"\"\"\n",
    "    Split the text content of Chamorro Sunday readings into individual sentences and return them along with their metadata.\n",
    "\n",
    "    This function processes a JSON dictionary containing the text content of multiple Chamorro Sunday readings, \n",
    "    splitting each reading's text into sentences. It also collects the title, source name, publication date, \n",
    "    and URL for each reading, and returns them as a list of tuples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        A dictionary where each key is an article identifier and each value contains metadata (title, date, URL) \n",
    "        and text content of the article.\n",
    "    source_name : str, optional\n",
    "        The name of the source from which the text content originates. Default is \"North Star Sunday Readings\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_sentences: list of tuples\n",
    "        A list of tuples, where each tuple contains the following elements:\n",
    "            - Sentence (str): A cleaned sentence from the article text.\n",
    "            - Title (str): The title of the article.\n",
    "            - Source (str): The source of the article (default is \"North Star Sunday Readings\").\n",
    "            - Date (str): The publication date of the article.\n",
    "            - URL (str): The URL of the article.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The text content is processed by removing non-breaking spaces (`\\xa0`), newline characters (`\\n`), \n",
    "      and extra whitespace. Sentences are tokenized using the `sent_tokenize` function from the `tokenize` module.\n",
    "    - The function returns the sentences as a list of tuples, with each tuple containing the sentence and its associated metadata.\n",
    "    \"\"\"\n",
    "    # Initialize list for storing tuples\n",
    "    all_sentences = []\n",
    "\n",
    "    # Iterate through data to get article text and metadata\n",
    "    for article_id, article_data in data.items():\n",
    "        # Get article text and metadata\n",
    "        title = article_data.get(\"title\", \"\")\n",
    "        date = article_data.get(\"date\", \"\")\n",
    "        url = article_data.get(\"url\", \"\")\n",
    "        text = article_data.get(\"text\", \"\")\n",
    "        \n",
    "        # Split text into sentences\n",
    "        sentences = tokenize.sent_tokenize(text)\n",
    "        \n",
    "        # Clean each sentence; remove tags, newlines and extra spaces\n",
    "        for s in sentences:\n",
    "            s = s.replace('\\xa0', ' ')\n",
    "            s = s.replace('\\n', ' ')\n",
    "            s = re.sub(r'\\s+', ' ', s)\n",
    "            s = s.strip()\n",
    "\n",
    "            # Append sentence and sentence metadata to all_sentences\n",
    "            all_sentences.append((s, title, source_name, date, url))\n",
    "    \n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "86b76e24-10e2-40f9-aaa1-e7d56a01ed4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20,514 total sentences\n"
     ]
    }
   ],
   "source": [
    "# Get sentences\n",
    "sentences = split_into_sentences(contents)\n",
    "total_sentences = len(sentences)\n",
    "print(f'{total_sentences:,} total sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35038c4b-c045-44e7-aa5d-11d4dc548b25",
   "metadata": {},
   "source": [
    "## Export Sentences to .CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "f6f022c1-c1ba-4673-b1c8-6d0fab44a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "df_sentences = pd.DataFrame(sentences, columns=[\"Sentence\", \"Title\", \"Author\", \"Date\", \"Source\"])\n",
    "\n",
    "# Save dataframe to CSV\n",
    "df_sentences.to_csv('sentences_sunday_readings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324af40-eaee-4c83-a1ad-c5ffc046946f",
   "metadata": {},
   "source": [
    "# Get Words for Lexicon Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef23709-938b-4160-bfc7-d8ae3a084b52",
   "metadata": {},
   "source": [
    "In this section we will generate a unique word list from the text content of the Sunday Readings and export the list to a .CSV file for future analysis and Chamorro lexicon expansion efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "38b9317d-c495-421c-8747-cdef66c2b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_words(data):\n",
    "    \"\"\"\n",
    "    Split the text content of Chamorro Sunday readings into individual words and return them in a unique list.\n",
    "\n",
    "    This function processes a JSON dictionary containing the text of multiple Chamorro Sunday readings and splits the text for each reading\n",
    "    into words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        A dictionary where each key is an article identifier and each value contains metadata (title, date, URL) \n",
    "        and text content of the article.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    word_list: list\n",
    "        A list where each element is a string.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The text content is processed by converting all characters to lowercase, removing punctuation and digits.\n",
    "    - The function returns a unique word list across all Chamorro Sunday readings\n",
    "    \"\"\"\n",
    "    # Initialize list for storing words\n",
    "    word_list = []\n",
    "\n",
    "    # Iterate through data, get text and append each word to words\n",
    "    for article_id, article_data in data.items():\n",
    "        text = article_data.get(\"text\", \"\") # Get text\n",
    "\n",
    "        # Do text cleanup\n",
    "        text = text.lower() # Convert all characters to lowercase\n",
    "        remove_chars = string.punctuation + string.digits # To remove common punctuation and numbers\n",
    "        text = text.translate(str.maketrans('','', remove_chars)) # Clean text\n",
    "        text = text.replace('”', '').replace('“','').replace('‘','') # To remove special quotes\n",
    "\n",
    "        # Split into words and append to word_list\n",
    "        words = text.split()\n",
    "        word_list += [word for word in words]\n",
    "\n",
    "    return list(set(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "db92e07e-ea96-4eb9-8e57-8704765e44c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8,890 unique words\n"
     ]
    }
   ],
   "source": [
    "# Get unique word list\n",
    "words = split_into_words(contents)\n",
    "unique_word_count = len(words)\n",
    "print(f'{unique_word_count:,} unique words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0206da89-0dbd-4488-9dbb-9eda4a24b222",
   "metadata": {},
   "source": [
    "## Export Word List to .CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "9af67f93-3969-44a4-b679-d2b9469e472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "df_words = pd.DataFrame({'words_sunday_readings': words})\n",
    "\n",
    "# Save df_words to csv\n",
    "df_words.to_csv('words_sunday_readings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
